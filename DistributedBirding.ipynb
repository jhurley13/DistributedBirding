{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistributedBirding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Given a list of names and street addresses, \n",
    "\n",
    "For each person:\n",
    "- create an individual species list\n",
    "- create a list of hotspots\n",
    "- Create a list of species that is unique to their 5MR (if any)\n",
    "\n",
    "Note: Assumes roughly 5-20 contacts. More will work but will tax eBird and Nominatim, plus there are some n-squared algorithms involved.\n",
    "\n",
    "## References\n",
    "Geocode with Python: How to Convert physical addresses to Geographic locations â†’ Latitude and Longitude, Abdishakur\n",
    "https://towardsdatascience.com/geocode-with-python-161ec1e62b89\n",
    "\n",
    "## Installation\n",
    "pip install geopandas  \n",
    "pip install geopy  \n",
    "pip install tqdm\n",
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import math\n",
    "import more_itertools\n",
    "import urllib3\n",
    "import typing\n",
    "\n",
    "import traceback\n",
    "\n",
    "from more_itertools import flatten\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "# import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For making map\n",
    "import geojson\n",
    "import folium\n",
    "from folium import plugins\n",
    "from geojson_rewind import rewind\n",
    "import webcolors\n",
    "\n",
    "# For making sample data for article\n",
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# For hotspot circles\n",
    "import geog\n",
    "\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "# For downloading shapes file\n",
    "from zipfile import ZipFile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xutilities\n",
    "import xruxidownload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Globals\n",
    "DEBUGGING = False\n",
    "\n",
    "DEFAULT_CIRCLE_RADIUS = 5 # Miles\n",
    "KM_PER_MILE = 1.60934\n",
    "MY5MR_PREFIX = 'my5mr-'\n",
    "DEFAULT_MAX_OBSERVERS = 2\n",
    "DEFAULT_DAYS_BACK = 14\n",
    "MAX_DAYS_BACK = 30\n",
    "\n",
    "EXTENSION_CSV = '.csv'\n",
    "EXTENSION_EXCEL = '.xlsx'\n",
    "\n",
    "NOMINATIM_USER_AGENT = 'DistributedBirding'\n",
    "\n",
    "# Example of things we might want to filter out\n",
    "# ['gull sp.',\n",
    "#  'Tree/Violet-green Swallow',\n",
    "#  'swallow sp.',\n",
    "#  'hummingbird sp.',\n",
    "#  \"Rufous/Allen's Hummingbird\",\n",
    "#  'warbler sp. (Parulidae sp.)',\n",
    "#  \"Sharp-shinned/Cooper's Hawk\",\n",
    "#  'hawk sp.',\n",
    "#  'woodpecker sp.',\n",
    "#  'Downy/Hairy Woodpecker',\n",
    "#  'Mallard (Domestic type)']\n",
    "\n",
    "DEFAULT_SPECIES_FILTER = ['.sp'] # or for example, ['.sp', '\\(', '/'] to eliminate all examples above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.ebird.org/v2/ref/hotspot/{{regionCode}}\n",
    "# location = 'US-CA-085,US-CA-081' # Santa Clara County, San Mateo County\n",
    "REGION_SANTA_CLARA_COUNTY = 'US-CA-085'\n",
    "REGION_SAN_MATEO_COUNTY = 'US-CA-081'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter-specific Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@rrfd/cookiecutter-data-science-organize-your-projects-atom-and-jupyter-2be7862f487e\n",
    "# Base Path\n",
    "base_path = Path.cwd()\n",
    "\n",
    "# Data paths\n",
    "data_path = base_path / 'data'\n",
    "raw_data_path = data_path / 'raw'\n",
    "interim_data_path = data_path / 'interim'\n",
    "processed_data_path = data_path / 'processed'\n",
    "external_data_path = data_path / 'external'\n",
    "\n",
    "# Reports paths\n",
    "reports_path = base_path / 'reports'\n",
    "figures_path = reports_path / 'figures'\n",
    "\n",
    "# Input paths\n",
    "contacts_name = 'Contacts'\n",
    "# We will append either '.csv' or '.xlsx' to contacts_base_path\n",
    "contacts_base_path = external_data_path\n",
    "\n",
    "# The Participants file defaults to the contacts file\n",
    "# If a reduced set of participants is desired, create a file with a Name column\n",
    "participants_base_path = external_data_path\n",
    "\n",
    "# The Parameters file has 3 columns: RegionCodes, Radius, MaxObservers\n",
    "# The name column is the \n",
    "parameters_name = 'Parameters'\n",
    "parameters_base_path = external_data_path\n",
    "\n",
    "# External data files for US County shape files and US State FIPS codes\n",
    "# If these are not found, a Parameters file is required with a list of region codes\n",
    "county_map_data_path = processed_data_path / 'tl_2017_us_county'\n",
    "county_shx_shapes = 'tl_2017_us_county.shx'\n",
    "county_shx_path = county_map_data_path / county_shx_shapes\n",
    "state_fips_master_path = external_data_path / 'state_fips_master.csv'\n",
    "\n",
    "# Interim paths\n",
    "# address_geocache_path = interim_data_path / 'address_geocache.csv'\n",
    "geolocation_cache_path = interim_data_path / 'GeolocationCache.csv'\n",
    "\n",
    "# Outputs paths\n",
    "unique_species_path = reports_path / f'{MY5MR_PREFIX}uniques.csv'\n",
    "sharpies_map_path = reports_path / f'{MY5MR_PREFIX}team_circles.html'\n",
    "hotspots_path = reports_path / f'{MY5MR_PREFIX}hotspots.csv'\n",
    "unique_hotspots_path = reports_path / f'{MY5MR_PREFIX}unique-hotspots.csv'\n",
    "sample_unique_species_path = reports_path / f'{MY5MR_PREFIX}sample_uniques.csv'\n",
    "updated_contacts_path = reports_path / f'{MY5MR_PREFIX}updated_contacts.csv'\n",
    "limited_path  = reports_path / f'{MY5MR_PREFIX}limited.csv'\n",
    "\n",
    "# Shape file paths\n",
    "shape_zip_path = raw_data_path / 'tl_2017_us_county.zip'\n",
    "shape_dest_dir = processed_data_path / 'tl_2017_us_county'\n",
    "        \n",
    "# Credentials\n",
    "eBirdCredential_path = '/Volumes/TSecure3/other/eBirdCredentials.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_project_paths():\n",
    "    default_mode = 0o755\n",
    "    data_path.mkdir(mode=default_mode, parents=False, exist_ok=True)\n",
    "    raw_data_path.mkdir(mode=default_mode, parents=False, exist_ok=True)\n",
    "    interim_data_path.mkdir(mode=default_mode, parents=False, exist_ok=True)\n",
    "    processed_data_path.mkdir(mode=default_mode, parents=False, exist_ok=True)\n",
    "    external_data_path.mkdir(mode=default_mode, parents=False, exist_ok=True)\n",
    "    reports_path.mkdir(mode=default_mode, parents=False, exist_ok=True)\n",
    "#     figures_path.mkdir(mode=default_mode, parents=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual way to find region:\n",
    "# Go to https://ebird.org/explore\n",
    "# Enter your county in the search field for \"Explore Regions\"\n",
    "# When the page opens, look at the URL. Your region is after \"region/\" and before \"?\", e.g. US-CA-085\n",
    "# https://ebird.org/region/US-CA-085?yr=all\n",
    "\n",
    "# A parameters file is not required if the regions can be deduced from the contacts list\n",
    "# This requires the US States shapes files and the US States FIPS code file\n",
    "def read_parameters(parameters_path = parameters_base_path) -> pd.DataFrame:\n",
    "    df = pd.DataFrame() # Failure mode\n",
    "\n",
    "    try:\n",
    "        # full path passed in \n",
    "        if parameters_path.is_file():\n",
    "            if parameters_path.suffix == EXTENSION_CSV:\n",
    "                return pd.read_csv(parameters_path, dtype=str).fillna('')\n",
    "            else:\n",
    "                xdtypes = {'Participants':str,'Region':str,\n",
    "                           'Radius':float, 'MaxObservers':int, 'DaysBack':int}\n",
    "                return pd.read_excel(parameters_path, header=0, dtypes=xdtypes).fillna('')\n",
    "        else:\n",
    "            csv_path = parameters_path / f'{parameters_name}{EXTENSION_CSV}'\n",
    "            excel_path = parameters_path / f'{parameters_name}{EXTENSION_EXCEL}'\n",
    "\n",
    "            if csv_path.exists():\n",
    "                print(f'Parameters from: {csv_path}')\n",
    "                df = pd.read_csv(csv_path, dtype=str).fillna('')\n",
    "            elif excel_path.exists():\n",
    "                print(f'Parameters from: {excel_path}')\n",
    "                df = pd.read_excel(excel_path, header=0).fillna('')\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    except Exception as ee:\n",
    "        pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_to_kilometers(mi: float):\n",
    "    return mi * KM_PER_MILE\n",
    "\n",
    "def get_recent_nearby_observations(latitude: float, longitude: float, radius: int = 9, back: int = 30):\n",
    "    # https://ebird.org/ws2.0/ref/hotspot/geo?lat=37.4407&lng=-122.0936&fmt=json&dist=6\n",
    "    recentobs = pd.DataFrame()\n",
    "    try:\n",
    "        if not math.isnan(latitude) and not math.isnan(longitude):\n",
    "            api_url_base = 'https://ebird.org/ws2.0/data/obs/geo/recent'\n",
    "            api_auth_header = {'X-eBirdApiToken': ebird_api_key}\n",
    "            params = {\n",
    "                'lat': str(latitude),\n",
    "                'lng': str(longitude),\n",
    "                'dist': str(radius),\n",
    "                'back': str(back),\n",
    "                'hotspot': 'True', # Should we limit only to hotspots?\n",
    "                'includeProvisional': 'True' \n",
    "            }\n",
    "\n",
    "            rr = requests.get(api_url_base, params=params, headers=api_auth_header, stream=True)\n",
    "            if rr.status_code == requests.codes.ok:\n",
    "                recentobs = pd.DataFrame(rr.json())\n",
    "            rr.raise_for_status()\n",
    "\n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "\n",
    "    return recentobs\n",
    "\n",
    "\n",
    "def get_observations(df, radius_miles = DEFAULT_CIRCLE_RADIUS, days_back = DEFAULT_DAYS_BACK):\n",
    "    radius_kms = miles_to_kilometers(radius_miles)\n",
    "\n",
    "    obs = []\n",
    "    print(f\"\\nObservations by all eBird users in individual's {radius_miles:.0f}MR\\n in past {days_back} days\")\n",
    "    print(f'{\"Name\":^30}{\"Location\":^30}{\"Observations\":>14}')          \n",
    "          \n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # assumes lat, lng exist\n",
    "            ro = get_recent_nearby_observations(float(row['latitude']), float(row['longitude']), radius_kms, days_back)\n",
    "            # Filter out some things\n",
    "            search_values = DEFAULT_SPECIES_FILTER\n",
    "\n",
    "            ro = ro[~ro.comName.str.contains('|'.join(search_values), case=True)]\n",
    "          \n",
    "            name, lat, lng = [row[col] for col in ['Name', 'latitude', 'longitude']]\n",
    "            obscount = ro.shape[0]\n",
    "            location = f'({lat:.5f}, {lng:.5f})'\n",
    "            print(f'{name:<30}{location:<30}{obscount:>14}')\n",
    "\n",
    "            obs.append(ro)\n",
    "        except Exception as ee:\n",
    "            print('get_observations', ee)\n",
    "            obs.append(pd.DataFrame())\n",
    "\n",
    "    print('')\n",
    "\n",
    "    return df.assign(observations=obs) # This is a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV file with names and addresses\n",
    "# Original columns in CSV file are:\n",
    "#     ['Name', 'Street1', 'Street2', 'City', 'State', 'Zip', 'Cell', 'Email', 'latitude', 'longitude']\n",
    "# although only ['Name', 'Street1', 'City', 'State', 'Zip', 'latitude', 'longitude'] are used right now\n",
    "\n",
    "def load_contacts(contacts_path):\n",
    "    contacts_csv_path, contacts_excel_path = contacts_base_path, contacts_base_path\n",
    "    # If a full path to the file is given, use that\n",
    "    if contacts_path.is_file():\n",
    "        if contacts_path.suffix == EXTENSION_CSV:\n",
    "            contacts_csv_path = contacts_path\n",
    "            contacts_excel_path = None\n",
    "        else:\n",
    "            # may still fail if weird file passed in\n",
    "            contacts_excel_path = contacts_path\n",
    "            contacts_csv_path = None\n",
    "    else:\n",
    "        contacts_csv_path = contacts_path / f'{contacts_name}{EXTENSION_CSV}'\n",
    "        contacts_excel_path = contacts_path / f'{contacts_name}{EXTENSION_EXCEL}'\n",
    "\n",
    "    df = None # Failure mode\n",
    "    if contacts_csv_path and contacts_csv_path.exists():\n",
    "        print(f'CSV: {contacts_csv_path}')\n",
    "        df = pd.read_csv(contacts_csv_path, dtype=str).fillna('')\n",
    "        df['Address'] = df[['Street1', 'City', 'State', 'Zip']].agg(' '.join, axis=1).apply(str.strip)\n",
    "    elif contacts_excel_path and contacts_excel_path.exists():\n",
    "        # Excel imports Zip Codes as numeric\n",
    "        print(f'Excel: {contacts_excel_path}')\n",
    "        xdtypes = {'Name':str, 'Zip':str, 'latitude':float, 'longitude':float }\n",
    "        df = pd.read_excel(contacts_excel_path, header=0, dtype=xdtypes).fillna('')\n",
    "        df['Zip'] = df['Zip'].astype(str)\n",
    "        df['Address'] = df[['Street1', 'City', 'State', 'Zip']].agg(' '.join, axis=1).apply(str.strip)\n",
    "    else:\n",
    "        raise UserWarning(f'No contacts file found in: {contacts_path}')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_update_contacts(contacts_path = contacts_base_path):\n",
    "    contacts = load_contacts(contacts_path)\n",
    "\n",
    "    contacts.latitude  = contacts.latitude.replace('', 0.0).astype(float, errors='ignore')\n",
    "    contacts.longitude = contacts.longitude.replace('', 0.0).astype(float, errors='ignore')\n",
    "\n",
    "    # These are the rows that need updating with geolocation information\n",
    "    # No birding on Null Island; a (0,0) means we have no geo info\n",
    "    update_mask = contacts.apply(lambda x: bool(x.Address.strip()) and (x.latitude + x.longitude == 0.0), axis=1)\n",
    "    \n",
    "    needsloc = contacts[update_mask].copy()\n",
    "\n",
    "    if not needsloc.empty:\n",
    "        newlocs = addresses_to_geolocations(needsloc.Address)\n",
    "\n",
    "        merged = pd.merge(contacts, newlocs, how='left', left_on=\"Address\", right_on=\"Address\")\n",
    "        for col in ['latitude', 'longitude']:\n",
    "            merged[col + '_x'] = merged[col + '_x'].replace('', '0.0').astype(float)\n",
    "            merged[col + '_y'] = merged[col + '_y'].replace(np.nan, 0.0).astype(float)\n",
    "            merged[col] = merged[col + '_x'] + merged[col + '_y']\n",
    "\n",
    "        contacts = merged.drop(['latitude_x', 'latitude_y', 'longitude_x', 'longitude_y'], axis=1)\n",
    "\n",
    "    contacts['latitude']  = contacts['latitude'].replace(np.nan, 0.0).astype(float)\n",
    "    contacts['longitude'] = contacts['longitude'].replace(np.nan, 0.0).astype(float)\n",
    "\n",
    "    return contacts\n",
    "\n",
    "\n",
    "def filter_participants(contacts, participants):\n",
    "    df = contacts[contacts.Name.isin(participants)] if participants else contacts\n",
    "    # We can't do much with out geocoords, so drop\n",
    "    drop_mask = df.apply(lambda x: (x.latitude + x.longitude == 0.0), axis=1)\n",
    "    no_coords = [x for x in list(df[drop_mask].Name) if x!='']\n",
    "    if no_coords:\n",
    "        print(f'No coordinates for: {\", \".join(no_coords)}')\n",
    "    return df[~drop_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached_geolocations():\n",
    "    # Address, latitude, longitude\n",
    "    cached_geolocations = pd.DataFrame()\n",
    "    try:\n",
    "        if geolocation_cache_path.is_file():\n",
    "            cached_geolocations = pd.read_csv(geolocation_cache_path, index_col=False)\n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "        pass\n",
    "    \n",
    "    return cached_geolocations.fillna('')\n",
    "\n",
    "\n",
    "# Don't call this directly -- use the cache when ever possible by calling addresses_to_geolocations\n",
    "def find_geolocations(addresses: list):\n",
    "    df = pd.DataFrame(addresses, columns=['Address'])\n",
    "    delay_seconds = 2.0\n",
    "    est_seconds = delay_seconds * len(df.index)\n",
    "\n",
    "    locator = Nominatim(user_agent = NOMINATIM_USER_AGENT)\n",
    "    display(df)\n",
    "    print(f'Finding geolocations for {df.shape[0]} contacts')\n",
    "    # 1 - convenient function to delay between geocoding calls\n",
    "    geocoder = RateLimiter(locator.geocode, min_delay_seconds=delay_seconds)\n",
    "    # 2- - create location column\n",
    "    df['location'] = df['Address'].progress_apply(geocoder)\n",
    "    # 3 - create longitude, laatitude and altitude from location column (returns tuple)\n",
    "    df['point'] = df['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "    # 4 - split point column into latitude, longitude and altitude columns\n",
    "    df[['latitude', 'longitude', 'altitude']] = pd.DataFrame(df['point'].tolist(), index=df.index)\n",
    "    na_values = {'latitude': 0.0, 'longitude': 0.0, 'altitude': 0.0}\n",
    "    \n",
    "    return df.fillna(value=na_values)\n",
    "\n",
    "def addresses_to_geolocations(addresses: list):\n",
    "    # Takes a list of addresses and returns geolocation information\n",
    "    # It looks in the cache file first and updates caches after a lookup\n",
    "    cached_geolocations = read_cached_geolocations()\n",
    "\n",
    "    if not cached_geolocations.empty:\n",
    "        cached_subframe = cached_geolocations[cached_geolocations.Address.isin(addresses)]\n",
    "        to_look_up = list(set(addresses) - set(cached_subframe.Address))\n",
    "    else:\n",
    "        to_look_up = list(set(addresses))\n",
    "        \n",
    "    if to_look_up:\n",
    "        found_locations = find_geolocations(to_look_up)\n",
    "        # Now update cached with info from the newly found geolocations\n",
    "        new_cache = pd.concat([cached_geolocations, found_locations], \n",
    "                  ignore_index=True).drop_duplicates(subset=['Address'], keep='last')\n",
    "        new_cache.to_csv(geolocation_cache_path, index=False)\n",
    "    else:\n",
    "        # Everything already in the cache\n",
    "        new_cache = cached_geolocations\n",
    "        \n",
    "    return new_cache[new_cache.Address.isin(addresses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call with prepare_unique_lists(birders_df2)\n",
    "def prepare_unique_lists(df):\n",
    "    # Create a list for each person of species that only they will see\n",
    "    dfobs, omdf = pd.DataFrame(), pd.DataFrame()\n",
    "    try:\n",
    "        only_me = []\n",
    "        dfobs = df[[not x.empty for x in list(df.observations)]].copy()\n",
    "        dfobs.reset_index(drop=True, inplace=True)\n",
    "        obslist = list(dfobs['observations'])\n",
    "        print(f'Rows with observations: {len(obslist)}')\n",
    "\n",
    "        species = [list(row['comName']) for row in obslist]\n",
    "        for index in range(len(obslist)):\n",
    "            mine = set(species[index])\n",
    "            bna = list(flatten(species[:index]+species[index+1:]))\n",
    "            others = set(bna)\n",
    "            mo = list(mine - others)\n",
    "            only_me.append(sorted(mo))\n",
    "\n",
    "        # Pad lists to max len to fix the \"raggedness\"\n",
    "        N = max([len(x) for x in only_me])\n",
    "        only_me_padded = [list(more_itertools.padded(x, '', N)) for x in only_me]\n",
    "        omdf = pd.DataFrame(only_me_padded).T\n",
    "        omdf.columns=dfobs.Name\n",
    "    except Exception as ee:\n",
    "        print('prepare_unique_lists', ee)\n",
    "        [print(type(x)) for x in list(df.observations)]\n",
    "\n",
    "    return dfobs, omdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_5mr_reports(contacts_path = contacts_base_path, unique_path = unique_species_path,\n",
    "                        participants = [], circle_radius = DEFAULT_CIRCLE_RADIUS, days_back = DEFAULT_DAYS_BACK):\n",
    "    participant_observations, contacts = None, None\n",
    "    try:\n",
    "        contacts = load_and_update_contacts(contacts_path)\n",
    "        pcontacts = filter_participants(contacts, participants)\n",
    "        participant_observations = get_observations(pcontacts, circle_radius, days_back)\n",
    "\n",
    "        dfobs, omdf = prepare_unique_lists(participant_observations)\n",
    "\n",
    "        ts = f'{pd.Timestamp.now().replace(microsecond=0)}'\n",
    "        prefix = MY5MR_PREFIX + re.sub(r'[ ]', '_', re.sub(r'[:]', '-', ts)) + '_'\n",
    "        for index, row in dfobs.iterrows():\n",
    "            fname = prefix + re.sub(r'[ ]', '',row.Name) + '.csv'\n",
    "            row['observations'].to_csv(reports_path / fname)\n",
    "\n",
    "        # Now write out single CSV with only_me data\n",
    "        omdf.to_csv(unique_path)\n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "        traceback.print_tb(ee.__traceback__)\n",
    "        raise\n",
    "        \n",
    "    return participant_observations, contacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circles_map(group_center_pt, radius_in_miles = DEFAULT_CIRCLE_RADIUS):\n",
    "    # See e.g. https://github.com/python-visualization/folium/blob/master/folium/vector_layers.py\n",
    "    # color is e.g. '#3388ff' or 'red'\n",
    "\n",
    "    # radius (float) â€“ Radius of the circle, in meters.\n",
    "    circle_radius = 1000 * miles_to_kilometers(radius_in_miles)\n",
    "\n",
    "    # Web colors. Drop 'maroon', 'olive' ebcause they look terrible on map\n",
    "    circle_colors = ['red', 'yellow', 'lime', 'green', 'aqua', 'teal', 'blue', 'navy', 'fuchsia', 'purple']\n",
    "    circle_colors_count = len(circle_colors)\n",
    "\n",
    "    mm = folium.Map(location=group_center_pt, zoom_start=11)\n",
    "    for ix, row in df5mr.iterrows():\n",
    "        location = (float(row['latitude']), float(row['longitude']))\n",
    "    #     print(location)\n",
    "        _ = mm.add_child(folium.vector_layers.Circle(location, circle_radius, stroke=1, \n",
    "                                                     color=circle_colors[ix % circle_colors_count], \n",
    "                                                     opacity=0.5, dash_array='4 1'))\n",
    "    _ = mm.add_child(folium.LayerControl())\n",
    "    mm.save(outfile=sharpies_map_path.as_posix())\n",
    "    return mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "def get_hotspots_for_region(regionCode: str) -> pd.DataFrame:\n",
    "    # https://ebird.org/ws2.0/ref/hotspot/geo?lat=37.4407&lng=-122.0936&fmt=json&dist=6\n",
    "    # https://api.ebird.org/v2/ref/hotspot/{{regionCode}}\n",
    "    headers = ['locid', 'r1', 'r2', 'r3', 'lat', 'lng', 'name', 'date', 'num']\n",
    "    results = pd.DataFrame()\n",
    "    try:\n",
    "        api_url_base = 'https://api.ebird.org/v2/ref/hotspot'\n",
    "        api_auth_header = {'X-eBirdApiToken': ebird_api_key}\n",
    "        url = f'{api_url_base}/{regionCode}'\n",
    "        params = None #{ 'maxResults' : 200}\n",
    "        rr = requests.get(url, stream=True) #params=params, headers=api_auth_header, \n",
    "        if rr.status_code == requests.codes.ok:\n",
    "            results = pd.read_csv(StringIO(rr.text), names = headers, index_col=False)\n",
    "        rr.raise_for_status()\n",
    "\n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_hotspots_for_region_cached(regionCode: str) -> pd.DataFrame:\n",
    "    fpath = processed_data_path / f'hotspots-{regionCode}.csv'\n",
    "    if not fpath.is_file():\n",
    "        hs_df = get_hotspots_for_region(regionCode)\n",
    "        hs_df.to_csv(fpath, index=False)\n",
    "        return hs_df\n",
    "    else:\n",
    "        return pd.read_csv(fpath, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotspot_data_for_regions(region_codes: list):\n",
    "    first_region, *remaining_regions = region_codes\n",
    "    combined = get_hotspots_for_region_cached(first_region)\n",
    "\n",
    "    for rc in remaining_regions:\n",
    "        region_hotspots = get_hotspots_for_region_cached(rc)\n",
    "        combined = pd.concat([combined, region_hotspots], ignore_index=True) #, ignore_index=True\n",
    "\n",
    "    # Some hotspots have NAN (e.g. Ocean View Park (ALA Co.))\n",
    "    combined['num'].fillna(0, inplace=True)\n",
    "\n",
    "    combined_hotspot_geometry = [Point(x, y) for x, y in zip(combined.lng, combined.lat)]\n",
    "    combined_hotspot_gdf = gpd.GeoDataFrame(combined, geometry=combined_hotspot_geometry)\n",
    "    center_pt = combined_hotspot_gdf.unary_union.convex_hull.centroid.coords[0][::-1]\n",
    "\n",
    "    return combined, combined_hotspot_gdf, center_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table with all hotspots for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hotspots_for_each_person(df5mr, hotspots_gdf, circle_radius_miles = DEFAULT_CIRCLE_RADIUS):\n",
    "#     hotspots_gdf - A geopandas.GeoDataFrame made from (lat,lng) tuples for each hotspot in the county\n",
    "    birder_geometry = [Point(x, y) for x, y in zip(df5mr.longitude.astype(float), df5mr.latitude.astype(float))]\n",
    "    birder_gdf = gpd.GeoDataFrame(df5mr, geometry=birder_geometry)\n",
    "\n",
    "    individual_hotspots = []\n",
    "    circle_radius = 1000*miles_to_kilometers(circle_radius_miles) # radius in meters  1000 * \n",
    "    # combined_hotspot_tuples = zip(hotspots_gdf.name, hotspots_gdf.geometry)\n",
    "    # Assumes geometry is single point\n",
    "    for birder_loc in birder_gdf.geometry:\n",
    "        combined_hotspot_tuples = zip(list(hotspots_gdf.name), list(hotspots_gdf.geometry))\n",
    "        hotspots = [name for name, pt2 in combined_hotspot_tuples if geog.distance(pt2, birder_loc) < circle_radius]\n",
    "        individual_hotspots.append(hotspots)\n",
    "        \n",
    "    # Pad lists to max len to fix the \"raggedness\"\n",
    "    N = max([len(x) for x in individual_hotspots])\n",
    "    individual_hotspots_padded = [list(more_itertools.padded(x, '', N)) for x in individual_hotspots]\n",
    "    ihdf = pd.DataFrame(individual_hotspots_padded).T\n",
    "    ihdf.columns=birder_gdf.Name\n",
    "    ihdf.to_csv(hotspots_path, index=False)\n",
    "    \n",
    "    return ihdf, individual_hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table with any hotspots unique to each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_hotspots_for_each_person(df5mr, individual_hotspots, circle_radius_miles = DEFAULT_CIRCLE_RADIUS):\n",
    "#     individual_hotspots - Returned from call to all_hotspots_for_each_person\n",
    "    only_my_hotspots = []\n",
    "    for index in range(len(individual_hotspots)):\n",
    "        mine = set(individual_hotspots[index])\n",
    "        bna = list(flatten(individual_hotspots[:index]+individual_hotspots[index+1:]))\n",
    "        others = set(bna)\n",
    "        mo = list(mine - others)\n",
    "        only_my_hotspots.append(sorted(mo))\n",
    "        \n",
    "    # Pad lists to max len to fix the \"raggedness\"\n",
    "    N = max([len(x) for x in only_my_hotspots])\n",
    "    only_my_hotspots_padded = [list(more_itertools.padded(x, '', N)) for x in only_my_hotspots]\n",
    "    omhdf = pd.DataFrame(only_my_hotspots_padded).T\n",
    "    omhdf.columns=birder_gdf.Name\n",
    "    omhdf.to_csv(unique_hotspots_path, index=False)\n",
    "        \n",
    "    return omhdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limited Observers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_limited_observers_table(df5mr, max_observers = DEFAULT_MAX_OBSERVERS):\n",
    "    '''\n",
    "    Each row in df5mr has an individual observer (Name) and a DataFrame with all observations\n",
    "    in their 5MR (observations). In the observations DataFrame we us the species name \"comName\"\n",
    "    and the location (i.e. hotspot) name \"locName\"\n",
    "    The output is a table where each row is a species that has only been seen in the 5MRs \n",
    "    for \"max_observers\" people or less. We use \"limited\" to denote these\n",
    "    '''\n",
    "    species_dict = {}\n",
    "    for ix, row in df5mr.iterrows():\n",
    "        # For each individual observation (for this person)\n",
    "        for jx, obs in row.observations.iterrows():\n",
    "            key = obs.comName\n",
    "            cur = species_dict.get(key, [])\n",
    "            val = (row.Name, obs.locName)\n",
    "            cur.append(val)\n",
    "            species_dict[key] = cur\n",
    "\n",
    "    limited_dict = dict(filter(lambda elem: len(elem[1]) <= max_observers, species_dict.items()))\n",
    "\n",
    "    limiteds = []\n",
    "    for key in sorted(limited_dict.keys()):\n",
    "        item = limited_dict[key]\n",
    "        line = [key]\n",
    "        for x,y in item:\n",
    "            line.append(x)\n",
    "            line.append(y)\n",
    "        limiteds.append(line)\n",
    "\n",
    "    cols = ['Species']\n",
    "    for ix in range(max_observers):\n",
    "        cols.append(f'Name {ix+1}')\n",
    "        cols.append(f'Location {ix+1}')\n",
    "    \n",
    "    limited_df = pd.DataFrame(limiteds, columns = cols)\n",
    "    limited_df.to_csv(limited_path, index=False)\n",
    "    \n",
    "    return limited_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Hotspot Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srgb_black  = '#000000'\n",
    "\n",
    "marker_color_names = [\n",
    "    'HotPink', 'Red', 'Orange', 'Yellow', 'Maroon', 'Plum',\n",
    "    'Magenta', 'BlueViolet', 'Indigo', 'Lime', 'ForestGreen', 'Cyan',\n",
    "    'Navy', 'Blue', 'Teal', 'Silver', 'cornflowerblue',\n",
    "    'indianred', 'indigo', 'ivory', 'khaki', 'lavender', 'lavenderblush', 'lawngreen', \n",
    "    'lemonchiffon', 'lightblue', 'lightcoral', 'lightcyan', 'lightgoldenrodyellow', \n",
    "    'lightgray', 'lightgrey', 'lightgreen', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue'    \n",
    "]\n",
    "\n",
    "marker_colors = dict([(colorname, webcolors.name_to_hex(colorname.lower())) for colorname in marker_color_names])\n",
    "\n",
    "\n",
    "def color_for_marker(marker_size):\n",
    "    colors = [marker_colors.get(mn, 'HotPink') for mn in ['lightgrey', 'lightcyan', 'cornflowerblue']]\n",
    "    return colors[marker_size-1]\n",
    "\n",
    "\n",
    "def create_hotspots_markers(hotspots_df):\n",
    "    # hotspots_df has columns ['locid', 'lat', 'lng', 'name', 'num', 'marker_size']\n",
    "    markers = folium.FeatureGroup(name=\"Markers\")\n",
    "\n",
    "    for index, row in hotspots_df.iterrows():\n",
    "        hname = row['name']\n",
    "        xpopup = folium.Popup(row.to_frame().to_html())\n",
    "        marker = folium.Marker(\n",
    "            location=[row['lat'], row['lng']],\n",
    "            popup=xpopup,\n",
    "            tooltip=hname,\n",
    "            icon=folium.plugins.BeautifyIcon(\n",
    "                icon='twitter',\n",
    "                icon_shape='marker',\n",
    "                text_color=srgb_black,  # actually icon color\n",
    "                background_color=color_for_marker(row['marker_size']),\n",
    "                border_width=row['marker_size']\n",
    "            )\n",
    "        )\n",
    "\n",
    "        markers.add_child(marker)\n",
    "\n",
    "    return markers\n",
    "\n",
    "\n",
    "def my_hotspots_map(my_center_pt, my_hotspots_df, my_path, radius_in_miles = 5):\n",
    "    # See e.g. https://github.com/python-visualization/folium/blob/master/folium/vector_layers.py\n",
    "    # color is e.g. '#3388ff' or 'red'\n",
    "\n",
    "    # radius (float) â€“ Radius of the circle, in meters.\n",
    "    circle_radius = 1000 * miles_to_kilometers(radius_in_miles)\n",
    "\n",
    "    # Web colors. Drop 'maroon', 'olive' ebcause they look terrible on map\n",
    "    circle_colors = ['red', 'yellow', 'lime', 'green', 'aqua', 'teal', 'blue', 'navy', 'fuchsia', 'purple']\n",
    "    circle_colors_count = len(circle_colors)\n",
    "\n",
    "    mm = folium.Map(location=my_center_pt, zoom_start=11)\n",
    "    \n",
    "    # Draw the circle\n",
    "    _ = mm.add_child(folium.vector_layers.Circle(my_center_pt, circle_radius, stroke=1, \n",
    "                                                     color='lime', \n",
    "                                                     opacity=0.5, dash_array='4 1'))\n",
    "    \n",
    "    _ = mm.add_child(create_hotspots_markers(my_hotspots_df))\n",
    "    _ = mm.add_child(folium.LayerControl())\n",
    "    mm.save(outfile=my_path.as_posix())\n",
    "    \n",
    "    return mm\n",
    "\n",
    "\n",
    "def individual_hotspot_maps(df5mr):\n",
    "    for my_name in df5mr.Name:\n",
    "        my_entry = df5mr[df5mr.Name==my_name].iloc[0] #['latitude', 'longitude']\n",
    "        my_center_pt = my_entry['latitude'], my_entry['longitude']\n",
    "\n",
    "        my_hotspots = sorted(list(set(ihdf[my_name])))\n",
    "        my_hotspots_df = hotspots[hotspots.name.isin(my_hotspots)].reset_index(drop=True).copy()[['locid', 'lat', 'lng', 'name', 'num']]\n",
    "\n",
    "        my_hotspots_df['marker_size'] = pd.qcut(my_hotspots_df['num'], q=3, labels=[1, 2, 3])\n",
    "\n",
    "        my_path = reports_path / f'{MY5MR_PREFIX}map_{my_name}.html'\n",
    "        my_radius_in_miles = 5\n",
    "\n",
    "        mm = my_hotspots_map(my_center_pt, my_hotspots_df, my_path, my_radius_in_miles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Region Codes from Contacts List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_us_county_shape_files():\n",
    "    #https://catalog.data.gov/dataset/tiger-line-shapefile-2017-nation-u-s-current-county-and-equivalent-national-shapefile\n",
    "    \n",
    "    try:\n",
    "        shape_zip_url = 'https://www2.census.gov/geo/tiger/TIGER2017/COUNTY/tl_2017_us_county.zip'\n",
    "        xruxidownload.download_file(shape_zip_url, shape_zip_path.as_posix(), False)   \n",
    "\n",
    "        shape_dest_dir = processed_data_path / 'tl_2017_us_county'\n",
    "        shape_dest_dir.mkdir(mode=0o744, parents=False, exist_ok=False)\n",
    "\n",
    "        # https://thispointer.com/python-how-to-unzip-a-file-extract-single-multiple-or-all-files-from-a-zip-archive/\n",
    "        with ZipFile(shape_zip_path, 'r') as zipObj:\n",
    "           # Extract all the contents of zip file in different directory\n",
    "           zipObj.extractall(shape_dest_dir)\n",
    "    \n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "        traceback.print_tb(ee.__traceback__)\n",
    "        raise UserWarning(f'Could not download US County shape files from: {shape_zip_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www2.census.gov/geo/tiger/TIGER2017/COUNTY/tl_2017_us_county.zip\n",
    "def load_us_county_data() -> (pd.DataFrame, pd.DataFrame):\n",
    "    # Load map data for counties in US. This takes a bit of time\n",
    "    # Both files are required, \n",
    "    map_us_counties, state_fips_master_df = pd.DataFrame(), pd.DataFrame() # Failure mode\n",
    "    \n",
    "    if not shape_dest_dir.is_dir():\n",
    "        download_us_county_shape_files()\n",
    "    \n",
    "    try:\n",
    "        map_us_counties = gpd.read_file(county_shx_path)\n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "        raise UserWarning(f'Missing US County data: {county_shx_path}')\n",
    "\n",
    "    try:\n",
    "        state_fips_master_df = pd.read_csv(state_fips_master_path, dtype=str).fillna('')\n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "        raise UserWarning(f'Missing US State FIPS data: {state_fips_master_path}')\n",
    "\n",
    "    return map_us_counties, state_fips_master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regionType: The region type: 'country', 'subnational1' or 'subnational2'.\n",
    "# parentRegionCode: The country or subnational1 code, or 'world'.\n",
    "# fmt: The format for the records, either 'csv' or 'json' (the default).\n",
    "# https://api.ebird.org/v2/ref/region/list/{{regionType}}/{{parentRegionCode}}.{{fmt}}\n",
    "\n",
    "def get_sub_region_list(parent_region_code: str, region_type: str):\n",
    "    # https://api.ebird.org/v2/ref/region/list/{{regionType}}/{{parentRegionCode}}.{{fmt}}\n",
    "    # e.g. get_sub_region_list('US', 'subnational1') for list of states\n",
    "    # e.g. get_sub_region_list('US-CA', 'subnational2') for county list\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "    try:\n",
    "        fmt = 'json'\n",
    "        api_url = f'https://api.ebird.org/v2/ref/region/list/{region_type}/{parent_region_code}.{fmt}'\n",
    "        api_auth_header = {'X-eBirdApiToken': ebird_api_key}\n",
    "\n",
    "        rr = requests.get(api_url, params=None, headers=api_auth_header, stream=True)\n",
    "        if rr.status_code == requests.codes.ok:\n",
    "            result = pd.DataFrame(rr.json())\n",
    "        rr.raise_for_status()\n",
    "\n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def region_codes_from_contacts(contacts: pd.DataFrame, map_us_counties, state_fips_master_df) -> pd.DataFrame:\n",
    "\n",
    "    def matching_row(row):\n",
    "        # row from srl has code\tname like US-SC-001\tAbbeville\n",
    "        return (row.code[3:5], row.xname) in list(zip(i2.state2, i2.NAME))\n",
    "\n",
    "    def get_2char_state_code(fips):\n",
    "        return str(state_fips_master_df[state_fips_master_df.fips==str(fips)].state_abbr.values[0])\n",
    "\n",
    "    region_codes_df = pd.DataFrame()\n",
    "    \n",
    "    contacts_geometry = [Point(x, y) for x, y in zip(contacts.longitude.astype(float), contacts.latitude.astype(float))]\n",
    "    contacts_gdf = gpd.GeoDataFrame(contacts, geometry=contacts_geometry)\n",
    "\n",
    "    contacts_gdf_x = gpd.GeoDataFrame(contacts_gdf, geometry='geometry',crs={'init' :'epsg:4269'})\n",
    "\n",
    "    contacts_gdf_y = contacts_gdf_x.copy()\n",
    "    contacts_gdf_y['geometry']=contacts_gdf_x['geometry'].buffer(5*(1/60))\n",
    "\n",
    "    intersections_df = pd.DataFrame()\n",
    "    for geo in contacts_gdf_y.geometry:\n",
    "        intersections_df = pd.concat([intersections_df, map_us_counties[map_us_counties.intersects(geo)]])\n",
    "\n",
    "    srl = get_sub_region_list('US', 'subnational2')\n",
    "    # Rename 'name' to 'xname' so we can use srl.xname (srl.name is reserved)\n",
    "    srl.columns = ['code', 'xname']\n",
    "\n",
    "    intersections_df['state2'] = intersections_df.STATEFP.astype(int).apply(get_2char_state_code)\n",
    "    i2 = intersections_df.copy()[['NAME', 'state2']].drop_duplicates().reset_index(drop=True)\n",
    "    region_codes_df = srl[srl.apply(matching_row, axis=1)].reset_index(drop=True)\n",
    "\n",
    "    return region_codes_df\n",
    "\n",
    "def get_region_codes_for_contacts(contacts, params) -> list:\n",
    "    # If the parameters file exists and has a list of region codes, use that\n",
    "    if not params.empty:\n",
    "        region_codes = [y for y in [x.strip() for x in params.Region] if y!='']\n",
    "        if region_codes:\n",
    "            return region_codes\n",
    "    \n",
    "    region_codes = []\n",
    "    map_us_counties, state_fips_master_df = load_us_county_data()\n",
    "\n",
    "    region_codes_df = region_codes_from_contacts(contacts, map_us_counties, state_fips_master_df)\n",
    "\n",
    "    return list(region_codes_df.code)\n",
    "\n",
    "\n",
    "def get_participants_from_params(params) -> list:\n",
    "    if not params.empty:\n",
    "        participants = [y for y in [x.strip() for x in params.Participants] if y!='']\n",
    "        return participants\n",
    "    else:\n",
    "        participants = [] # really all\n",
    "        \n",
    "\n",
    "def get_numeric_from_params(params, column_name, defaultval):\n",
    "    val = defaultval\n",
    "    try:\n",
    "        if not params.empty:\n",
    "            val = params[column_name].values[0]\n",
    "    except Exception as ee:\n",
    "        pass\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "credentials = xutilities.load_credentials(eBirdCredential_path)['credentials']\n",
    "ebird_api_key = credentials['api_key']\n",
    "\n",
    "# For geocoder progress bar (see progress_apply)\n",
    "tqdm.pandas()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    create_project_paths()\n",
    "    \n",
    "    try:\n",
    "        parameters_path = parameters_base_path\n",
    "        if DEBUGGING:\n",
    "            # Comment out one by one\n",
    "            sample_path = base_path / 'SampleInputs'\n",
    "            parameters_path = sample_path / 'Parameters-Default.xlsx'\n",
    "            parameters_path = sample_path / 'Parameters-Regions.xlsx'\n",
    "#             parameters_path = sample_path / 'Parameters-Participants.xlsx'\n",
    "\n",
    "        params = read_parameters(parameters_path)\n",
    "        maxobservers = int(get_numeric_from_params(params, 'MaxObservers', DEFAULT_MAX_OBSERVERS))\n",
    "        circle_radius = float(get_numeric_from_params(params, 'Radius', DEFAULT_CIRCLE_RADIUS))\n",
    "        days_back = int(get_numeric_from_params(params, 'DaysBack', DEFAULT_DAYS_BACK))\n",
    "        if days_back >= 0 or days_back > MAX_DAYS_BACK:\n",
    "            days_back = DEFAULT_DAYS_BACK\n",
    "        \n",
    "        contacts_path = contacts_base_path\n",
    "        if DEBUGGING:\n",
    "            # Comment out one by one\n",
    "            sample_path = base_path / 'SampleInputs'\n",
    "            contacts_path = sample_path / 'Contacts-Sample-crosscountry.xlsx'\n",
    "            contacts_path = sample_path / 'Contacts-Sample-missing.xlsx'\n",
    "            contacts_path = sample_path / 'Contacts-Sample.xlsx'\n",
    "\n",
    "        participants = get_participants_from_params(params)\n",
    "        df5mr, contacts = generate_5mr_reports(contacts_path, unique_species_path, \n",
    "                                               participants, circle_radius, days_back)\n",
    "    \n",
    "        region_codes = get_region_codes_for_contacts(contacts, params)\n",
    "        print(f'Region codes: {region_codes}')\n",
    "        hotspots, hotspot_gdf, hs_center_pt = hotspot_data_for_regions(region_codes)\n",
    "\n",
    "        birder_geometry = [Point(x, y) for x, y in zip(df5mr.longitude.astype(float), df5mr.latitude.astype(float))]\n",
    "        birder_gdf = gpd.GeoDataFrame(df5mr, geometry=birder_geometry)\n",
    "\n",
    "        # Reverse the order of the point; Shapely uses (lng, lat) but folium is (lat, lng)\n",
    "        # This point is used to center the map\n",
    "        group_center_pt = birder_gdf.unary_union.convex_hull.centroid.coords[0][::-1]\n",
    "\n",
    "        mm = create_circles_map(group_center_pt, circle_radius)\n",
    "\n",
    "        ihdf, individual_hotspots = all_hotspots_for_each_person(df5mr, hotspot_gdf, circle_radius)\n",
    "        uhdf = unique_hotspots_for_each_person(df5mr, individual_hotspots, circle_radius)\n",
    "        limited_df = create_limited_observers_table(df5mr, maxobservers)\n",
    "\n",
    "        individual_hotspot_maps(df5mr)\n",
    "        \n",
    "    except Exception as ee:\n",
    "        print(ee)\n",
    "        traceback.print_tb(ee.__traceback__)\n",
    "\n",
    "\n",
    "    print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
